{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from PIL import Image\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac62ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9efc979",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2431d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"microsoft/Florence-2-base-ft\"\n",
    "DEVICE   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DTYPE    = torch.float16 if DEVICE == \"cuda\" else torch.float32\n",
    "\n",
    "# 1) Load Florence processor and model once\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "bnb = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    # bnb_4bit_use_double_quant=True,\n",
    "    # bnb_4bit_quant_type=\"nf4\",\n",
    "    # bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "# load the model in the quantization specified above \n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, trust_remote_code=True, quantization_config=bnb, low_cpu_mem_usage=True).eval()\n",
    "\n",
    "model.to(DEVICE) # move 4-bit model to GPU \n",
    "\n",
    "print(f\"device_maps: {model.hf_device_map}\")\n",
    "\n",
    "def get_vram(): \n",
    "    if torch.cuda.is_available(): \n",
    "        print(f'Memory summary: {torch.cuda.memory_summary()}')\n",
    "    else: \n",
    "        print(\"cuda unavailable\")\n",
    "\n",
    "def get_targeted_mem_stats():\n",
    "    if torch.cuda.is_available(): \n",
    "        print(f'Total allocated: {torch.cuda.memory_allocated() / 1024 / 1024} MB')\n",
    "        print(f'Total reserved: {torch.cuda.memory_reserved() / 1024 / 1024} MB')\n",
    "\n",
    "def _warmup(img):\n",
    "    \"\"\"Optional one-time warmup to stabilize kernel caching / allocator behavior.\"\"\"\n",
    "    print(\"Warming up model...\")\n",
    "    inputs = processor(text=\"Caption\", images=img, return_tensors=\"pt\").to(DEVICE, DTYPE)\n",
    "    with torch.inference_mode():\n",
    "        _ = model.generate(**inputs, max_new_tokens=8)\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    print(\"Warmup complete.\\n\")\n",
    "\n",
    "def capture_frame(max_side=640):\n",
    "    \"\"\"Capture one webcam frame and return a resized PIL.Image.\"\"\"\n",
    "    cap = cv2.VideoCapture(0, cv2.CAP_DSHOW)  # DSHOW = faster on Windows\n",
    "    cap.set(cv2.CAP_PROP_FRAME_WIDTH,  640) # frame width and heigh in pixels do i need to change? \n",
    "    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "    t0 = time.time()\n",
    "\n",
    "    ok, frame = cap.read()\n",
    "    cap.release()\n",
    "\n",
    "    if not ok:\n",
    "        raise RuntimeError(\"Could not read from webcam\")\n",
    "\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    h, w = frame.shape[:2]\n",
    "    scale = max_side / max(h, w)\n",
    "    if scale < 1.0:\n",
    "        frame = cv2.resize(frame, (int(w*scale), int(h*scale)), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "    print(f\"[capture] {time.time() - t0:.3f}s\")\n",
    "    return Image.fromarray(frame)\n",
    "\n",
    "def caption_image(img, max_new_tokens=24):\n",
    "    \"\"\"Run greedy captioning on the GPU using Florence-2.\"\"\"\n",
    "    inputs = processor(text='<CAPTION>', images=img, return_tensors=\"pt\").to(DEVICE, DTYPE)\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.inference_mode():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            do_sample=False,\n",
    "            num_beams=1,        # greedy\n",
    "            use_cache=True,\n",
    "            early_stopping=False\n",
    "        )\n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.synchronize()\n",
    "    gen_time = time.time() - t0\n",
    "    print(f\"[generate] {gen_time:.3f}s\")\n",
    "\n",
    "    t1 = time.time()\n",
    "    text = processor.batch_decode(out, skip_special_tokens=True)[0]\n",
    "    print(f\"[decode] {time.time() - t1:.3f}s\")\n",
    "    return text\n",
    "\n",
    "def reset_cache():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "def main():\n",
    "    # Optional warmup on a single frame to stabilize timings\n",
    "    reset_cache()\n",
    "    \n",
    "    print(\"VRAM usage after cache clear\")\n",
    "    # get_vram()\n",
    "    get_targeted_mem_stats()\n",
    "\n",
    "    warmup_img = capture_frame()\n",
    "    _warmup(warmup_img)\n",
    "\n",
    "    # reset_cache()\n",
    "    print(\"Starting Florence captioning loop. Press 'q' in the console to quit.\\n\")\n",
    "    print(\"VRAM usage after loading\")\n",
    "    # get_vram()\n",
    "    get_targeted_mem_stats()\n",
    "    \n",
    "    while True:\n",
    "        img = capture_frame(max_side=512)\n",
    "        caption = caption_image(img)\n",
    "        print(f\"üìù Caption: {caption}\\n\")\n",
    "\n",
    "        # Non-blocking check for user quit\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\"Quitting...\")\n",
    "            break\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20695989",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Any param device:\", next(model.parameters()).device)  # should be cuda:0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b048ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
