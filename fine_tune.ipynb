{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3692db2d",
   "metadata": {},
   "source": [
    "### Push dataset to Huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e0995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import os \n",
    "\n",
    "api = HfApi(token=os.getenv(\"HF_TOKEN\"))\n",
    "api.upload_folder(\n",
    "    folder_path=\"data/rheo_sigmoid\",\n",
    "    repo_id=\"dchip95/synthetic-oscillatory-rheology-vlm\",\n",
    "    repo_type=\"dataset\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36428f9a",
   "metadata": {},
   "source": [
    "##### push the train and validation folders to the Hub for future loads and sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b8df76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataset dict and then push to HF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92779eb6",
   "metadata": {},
   "source": [
    "##### check GPU VRAM and clear if in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available() and (torch.cuda.memory_allocated() // 1024 // 1024) > 10: \n",
    "    print(f\"{torch.cuda.memory_allocated() // 1024 // 1024} MB currently allocated\")\n",
    "    print(f\"{torch.cuda.memory_reserved() // 1024 // 1024} MB currently reserved\")\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.reset_peak_memory_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633a608c",
   "metadata": {},
   "source": [
    "#### Import libraries and set torch device properties "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4312411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BitsAndBytesConfig, AutoModelForCausalLM, AutoProcessor\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "%matplotlib inline\n",
    "\n",
    "# model_id = \"OpenGVLab/InternVL3-2B\"\n",
    "model_id = 'microsoft/Florence-2-base-ft'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# florence 2 \n",
    "# load processor \n",
    "# inputs loaded into processor \n",
    "# processor processed inputs to go to model\n",
    "# model object calls generate()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(load_in_4bit=True,)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                quantization_config=bnb_config,\n",
    "                                low_cpu_mem_usage=True, \n",
    "                                trust_remote_code=True).eval()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_id, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9ea733",
   "metadata": {},
   "source": [
    "#### From https://huggingface.co/blog/finetune-florence2 they freeze the vision encoder to make fine tuning less expensive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb25a227",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.vision_tower.parameters(): \n",
    "    param.is_trainable = False "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef94263",
   "metadata": {},
   "source": [
    "#### Now we can begin the finetune process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbd4bb8b",
   "metadata": {},
   "source": [
    "#### Format the image and label paths with sample_ prefix for uniformity and clarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from pathlib import Path \n",
    "\n",
    "train_images = Path('data/rheo_sigmoid/train/images')\n",
    "train_lables = Path('data/rheo_sigmoid/train/labels')\n",
    "val_images = Path('data/rheo_sigmoid/val/images')\n",
    "val_labels = Path('data/rheo_sigmoid/val/labels')\n",
    "\n",
    "img_ext = '.png'\n",
    "label_ext = '.json'\n",
    "\n",
    "prefix = 'sample_'\n",
    "\n",
    "data_dict = {val_images: '.png', \n",
    "             val_labels: '.json', \n",
    "             train_images: '.png', \n",
    "             train_lables: '.json'\n",
    "             }\n",
    "\n",
    "for path_, ext in data_dict.items(): \n",
    "    for f in path_.glob(f'*{ext}'): \n",
    "        if f.is_file(): \n",
    "            new_file_name = f.parent / Path(prefix + f.name)\n",
    "            f.rename(new_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c0d41",
   "metadata": {},
   "source": [
    "#### create a jsonl file with the following structure \n",
    "{id: sample_123, \n",
    "image_path: Path, \n",
    "prompt: \"You are....\", \n",
    "target: json_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "from pathlib import Path\n",
    "import json \n",
    "\n",
    "\n",
    "def generate_jsonl_inputs(img_path, label_path): \n",
    "\n",
    "    imgs = [f.as_posix() for f in img_path.glob('*') if f.is_file()] # as_posix() convers windows path object to forward slashed string \n",
    "\n",
    "    targets = [] # empty list of dict to append targets_interim dicts \n",
    "\n",
    "    sample_ids = []\n",
    "\n",
    "    for f in label_path.glob('*.json'): \n",
    "\n",
    "        targets_interim = []\n",
    "        deserialized_data = json.loads(f.read_text())\n",
    "\n",
    "        sample_ids.append(deserialized_data['figure_id'])\n",
    "\n",
    "        for i in range(len(deserialized_data['materials'])): \n",
    "\n",
    "            data = {'legend_entry': deserialized_data['materials'][i]['label_in_legend'], \n",
    "                    'Gp_plateau_Pa': math.floor(deserialized_data['materials'][i]['Gp_plateau_Pa']),\n",
    "                    'tau_y_Pa': math.floor(deserialized_data['materials'][i]['tau_y_Pa']), \n",
    "                    'tay_f_Pa': math.floor(deserialized_data['materials'][i]['tau_f_Pa'])\n",
    "            }\n",
    "\n",
    "            targets_interim.append(data)\n",
    "        \n",
    "        targets.append(targets_interim)\n",
    "\n",
    "    return sample_ids, imgs, targets\n",
    "\n",
    "\n",
    "def generate_jsonl_dict(sample_ids, imgs, prompt, targets): \n",
    "\n",
    "    keys = ['ids', 'image_path', 'prompt', 'target']\n",
    "    prompt_verbiage = prompt * len(sample_ids)\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(sample_ids)): \n",
    "        values = [sample_ids[i], imgs[i], prompt_verbiage[i], targets[i]]\n",
    "        data.append(dict(zip(keys, values)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_jsonl(output_file, data): \n",
    "\n",
    "    with open(output_file, 'w') as f: \n",
    "        for item in data: \n",
    "            json_line = json.dumps(item)\n",
    "            f.write(json_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6799fd48",
   "metadata": {},
   "source": [
    "##### generate train.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5823d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"You are a rheology assistant. Extract the rheological parameters and respond strictly in json.\"]\n",
    "\n",
    "# train.jsonl \n",
    "train_images = Path('data/rheo_sigmoid/train/images')\n",
    "train_labels = Path('data/rheo_sigmoid/train/labels')\n",
    "\n",
    "train_dir = Path('data/rheo_sigmoid/train')\n",
    "train_file = train_dir / 'train.jsonl'\n",
    "\n",
    "sample_ids, imgs, targets = generate_jsonl_inputs(img_path=train_images, label_path=train_labels)\n",
    "train_data = generate_jsonl_dict(sample_ids=sample_ids, imgs=imgs, targets=targets, prompt=prompt)\n",
    "generate_jsonl(train_file, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a80ae95",
   "metadata": {},
   "source": [
    "#### generate val.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3022821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.jsonl\n",
    "val_images = Path('data/rheo_sigmoid/val/images')\n",
    "val_labels = Path('data/rheo_sigmoid/val/labels')\n",
    "\n",
    "val_dir = Path('data/rheo_sigmoid/val')\n",
    "val_file = val_dir / 'val.jsonl'\n",
    "\n",
    "sample_ids, imgs, targets = generate_jsonl_inputs(img_path=val_images, label_path=val_labels)\n",
    "val_data = generate_jsonl_dict(sample_ids=sample_ids, imgs=imgs, targets=targets, prompt=prompt)\n",
    "generate_jsonl(val_file, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f280f93",
   "metadata": {},
   "source": [
    "#### setup pytorch data class and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006e047c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset \n",
    "import json \n",
    "from PIL import Image\n",
    "\n",
    "class RheologyDataset(Dataset): \n",
    "    def __init__(self, data_path):\n",
    "        self.data = []\n",
    "        with open(data_path, 'rb') as f: \n",
    "            for line in f: \n",
    "                self.data.append(json.loads(line.strip()))\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx): \n",
    "        # access the ith dictionary and retrieve the img_path, prompt, and target string\n",
    "        sample = self.data[idx]\n",
    "        \n",
    "        # img is a string object and will need loaded as img first\n",
    "        img = Image.open(sample['image_path']).convert(\"RGB\")\n",
    "        prompt = sample['prompt']\n",
    "        target = sample['target']\n",
    "\n",
    "        return img, prompt, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1177008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm \n",
    "from transformers import AdamW, get_scheduler\n",
    "from pathlib import Path\n",
    "\n",
    "# collate function to utilize Florence2 processor on the individual samples within the batch\n",
    "# given the variation in image size, target token size, prompt variation, etc. \n",
    "def collate_fn(batch): \n",
    "    images, prompts, targets = zip(*batch)\n",
    "    inputs = processor(text=list(prompts), images=list(images), return_tensors=\"pt\", padding=True).to(device)\n",
    "    return inputs, targets\n",
    "\n",
    "train_file = Path('data/rheo_sigmoid/train/train.jsonl')\n",
    "val_file = Path('data/rheo_sigmoid/val/val.jsonl')\n",
    "\n",
    "train_dataset = RheologyDataset(train_file)\n",
    "val_dataset = RheologyDataset(val_file)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset=val_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
