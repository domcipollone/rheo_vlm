{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92779eb6",
   "metadata": {},
   "source": [
    "##### This notebook processess the synthetic rheology data JSON and corresponding plot into a Huggingface Dataset and DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b500781",
   "metadata": {},
   "source": [
    "##### push_to_hub accomplished by a huggingface-cli login      \n",
    "run hf auth whoami to confirm login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315b8548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "from pathlib import Path \n",
    "import math \n",
    "from huggingface_hub import notebook_login\n",
    "from PIL import Image \n",
    "from datasets import Dataset, DatasetDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42066cc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fbd4bb8b",
   "metadata": {},
   "source": [
    "##### Format the image and label paths with sample_ prefix for uniformity and clarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fba33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = Path('data/rheo_sigmoid/train/images')\n",
    "train_lables = Path('data/rheo_sigmoid/train/labels')\n",
    "val_images = Path('data/rheo_sigmoid/val/images')\n",
    "val_labels = Path('data/rheo_sigmoid/val/labels')\n",
    "\n",
    "img_ext = '.png'\n",
    "label_ext = '.json'\n",
    "\n",
    "prefix = 'sample_'\n",
    "\n",
    "data_dict = {val_images: '.png', \n",
    "             val_labels: '.json', \n",
    "             train_images: '.png', \n",
    "             train_lables: '.json'\n",
    "             }\n",
    "\n",
    "for path_, ext in data_dict.items(): \n",
    "    for f in path_.glob(f'*{ext}'): \n",
    "        if f.is_file(): \n",
    "            new_file_name = f.parent / Path(prefix + f.name)\n",
    "            f.rename(new_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223c0d41",
   "metadata": {},
   "source": [
    "#### create a jsonl file with the following structure \n",
    "{id: sample_123, \n",
    "image_path: Path, \n",
    "prompt: \"You are....\", \n",
    "target: json_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda216cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_jsonl_inputs(img_path, label_path): \n",
    "\n",
    "    imgs = [f.as_posix() for f in img_path.glob('*') if f.is_file()] # as_posix() converts windows path object to forward slashed string \n",
    "\n",
    "    targets = [] # empty list of dict to append targets_interim dicts \n",
    "\n",
    "    sample_ids = []\n",
    "\n",
    "    for f in label_path.glob('*.json'): \n",
    "\n",
    "        targets_interim = []\n",
    "        deserialized_data = json.loads(f.read_text())\n",
    "\n",
    "        sample_ids.append(deserialized_data['figure_id'])\n",
    "\n",
    "        for i in range(len(deserialized_data['materials'])): \n",
    "\n",
    "            data = {'legend_entry': deserialized_data['materials'][i]['label_in_legend'], \n",
    "                    'Gp_plateau_Pa': math.floor(deserialized_data['materials'][i]['Gp_plateau_Pa']),\n",
    "                    'tau_y_Pa': math.floor(deserialized_data['materials'][i]['tau_y_Pa']), \n",
    "                    'tay_f_Pa': math.floor(deserialized_data['materials'][i]['tau_f_Pa'])\n",
    "            }\n",
    "\n",
    "            targets_interim.append(data)\n",
    "        \n",
    "        targets.append(targets_interim)\n",
    "\n",
    "    return sample_ids, imgs, targets\n",
    "\n",
    "\n",
    "def generate_jsonl_dict(sample_ids, imgs, prompt, targets): \n",
    "\n",
    "    keys = ['ids', 'image_path', 'prompt', 'target']\n",
    "    prompt_verbiage = prompt * len(sample_ids)\n",
    "    data = []\n",
    "\n",
    "    for i in range(len(sample_ids)): \n",
    "        values = [sample_ids[i], imgs[i], prompt_verbiage[i], targets[i]]\n",
    "        data.append(dict(zip(keys, values)))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def generate_jsonl(output_file, data): \n",
    "\n",
    "    with open(output_file, 'w') as f: \n",
    "        for item in data: \n",
    "            json_line = json.dumps(item)\n",
    "            f.write(json_line + '\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5823d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"You are a rheology assistant. What are the storage modulus, yield stress, and flow stress for each material in the rheology plot? Extract the rheological parameters and respond strictly in json.\"]\n",
    "\n",
    "# train.jsonl \n",
    "train_images = Path('data/rheo_sigmoid/train/images')\n",
    "train_labels = Path('data/rheo_sigmoid/train/labels')\n",
    "\n",
    "train_dir = Path('data/rheo_sigmoid/train')\n",
    "train_file = train_dir / 'train.jsonl'\n",
    "\n",
    "sample_ids, imgs, targets = generate_jsonl_inputs(img_path=train_images, label_path=train_labels)\n",
    "train_data = generate_jsonl_dict(sample_ids=sample_ids, imgs=imgs, targets=targets, prompt=prompt)\n",
    "generate_jsonl(train_file, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3022821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val.jsonl\n",
    "val_images = Path('data/rheo_sigmoid/val/images')\n",
    "val_labels = Path('data/rheo_sigmoid/val/labels')\n",
    "\n",
    "val_dir = Path('data/rheo_sigmoid/val')\n",
    "val_file = val_dir / 'val.jsonl'\n",
    "\n",
    "sample_ids, imgs, targets = generate_jsonl_inputs(img_path=val_images, label_path=val_labels)\n",
    "val_data = generate_jsonl_dict(sample_ids=sample_ids, imgs=imgs, targets=targets, prompt=prompt)\n",
    "generate_jsonl(val_file, val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d962325",
   "metadata": {},
   "source": [
    "Quickly realized that i loaded my local image paths in the jsonl files  \n",
    "Push to Hub Dataset creation for loading in Colab  \n",
    "Removes dependency on local filepaths "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd907971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and val .jsonl files\n",
    "# load the image from the image_path and insert the PIL image\n",
    "# push that dataset in RAM to HF\n",
    "\n",
    "# use the Dataset.from_list(List) method\n",
    "# read the .jsonl \n",
    "train_file = 'data/rheo_sigmoid/train/train.jsonl'\n",
    "val_file = 'data/rheo_sigmoid/val/val.jsonl'\n",
    "\n",
    "prompt = [\"You are a rheology assistant. What are the storage modulus, yield stress, and flow stress for each material in the rheology plot? Extract the rheological parameters and respond strictly in json.\"]\n",
    "\n",
    "def read_json_lines(json_file):\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            yield json.loads(line) # dont need strip since load in as text\n",
    "\n",
    "\n",
    "def generate_hf_datset(jsonl_file):\n",
    "    data_struct = []\n",
    "    keys = ['id', 'image', 'prompt', 'target']\n",
    "\n",
    "    for sample in read_json_lines(jsonl_file):\n",
    "\n",
    "        img = Image.open(sample['image_path']).convert(\"RGB\")\n",
    "        target = json.dumps(sample['target'])\n",
    "\n",
    "        values = [sample['ids'], img, prompt, target]\n",
    "\n",
    "        data_struct.append(dict(zip(keys, values)))\n",
    "\n",
    "    return data_struct\n",
    "\n",
    "\n",
    "train_data_hf = generate_hf_datset(train_file)\n",
    "val_data_hf = generate_hf_datset(val_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd201c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create huggingface dataset dict using from_list() method (super convenient :))\n",
    "train_data_hf_dict = Dataset.from_list(train_data_hf)\n",
    "val_data_hf_dict = Dataset.from_list(val_data_hf)\n",
    "\n",
    "rheo_ds_dict = DatasetDict({'train': train_data_hf_dict, \n",
    "                            'validation': val_data_hf_dict}\n",
    "                            )\n",
    "\n",
    "# ensure login to hugginface-cli\n",
    "rheo_ds_dict.push_to_hub(\"dchip95/rheology_dataset_pixels\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
